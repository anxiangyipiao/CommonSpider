{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup,Tag\n",
    "import requests \n",
    "import re  \n",
    "\n",
    "\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299'\n",
    "\n",
    "}\n",
    "\n",
    "#  替换字符串\n",
    "def replace_str(source_str, regex, replace_str = ''):         \n",
    "     \n",
    "    str_info = re.compile(regex)         \n",
    "    return str_info.sub(replace_str, source_str)\n",
    "\n",
    "# 判断soup的子节点长度是否为1，是则递归调用，直到子节点长度不为1，返回soup\n",
    "def is_single_child_data(soup):\n",
    "    \n",
    "    children = list(soup.children)\n",
    "    num_children = [child for child in children if child.name is not None]        \n",
    "    if len(num_children) == 1:                  \n",
    "        return is_single_child_data(num_children[0])          \n",
    "    else:          \n",
    "        return soup  \n",
    "\n",
    "# 获得soup最长的孩子节点\n",
    "def max_child_data(soup):\n",
    "        \n",
    "    max_child = None \n",
    "    max_length = 0 \n",
    "\n",
    "    # 遍历soup的所有直接子节点     \n",
    "    for child in soup.children:         \n",
    "        if isinstance(child, Tag):  \n",
    "            # 确保它是一个标签             \n",
    "            text_length = len(child.get_text(strip=True))  # 获取去除空白后的文本长度             \n",
    "            if text_length > max_length:  \n",
    "                # 检查这是否是目前找到的最长的文本                 \n",
    "                max_length = text_length                 \n",
    "                max_child = child      \n",
    "    \n",
    "    return max_child  # 返回文本最长的子节点，如果没有找到则返回 None\n",
    "\n",
    "# 递归处理soup\n",
    "def recursive_process(soup):     \n",
    "    # 检查是否只有一个子节点，如果是则深入     \n",
    "    soup = is_single_child_data(soup)  \n",
    "\n",
    "    if is_item(soup):         \n",
    "        # 如果是我们想要的item，则返回soup         \n",
    "        return soup\n",
    "  \n",
    "    \n",
    "    # 在当前层找到最长的子节点     \n",
    "    new_soup = max_child_data(soup)   \n",
    "       \n",
    "    return recursive_process(new_soup)\n",
    "\n",
    "# re正则处理html\n",
    "def re_rules(response):\n",
    "\n",
    "\n",
    "    # 正则获得body里的内容\n",
    "    html = re.search(r'<body.*</body>', response.text, re.S).group(0)\n",
    "    # 去掉script标签\n",
    "    html = replace_str(html, '(?i)<script(.|\\n)*?</script>') #(?i)忽略大小写   \n",
    "    # 去掉style标签    \n",
    "    html = replace_str(html, '(?i)<style(.|\\n)*?</style>')\n",
    "    # 去掉form标签(有些数据放到form里，坑！)\n",
    "    # html = replace_str(html, '(?i)<form(.|\\n)*?</form>') \n",
    "    # 去掉option标签\n",
    "    html = replace_str(html, '(?i)<option(.|\\n)*?</option>')\n",
    "    # 去掉input标签\n",
    "    html = replace_str(html, '(?i)<input(.|\\n)*?>') \n",
    "    # 去掉img标签\n",
    "    html = replace_str(html, '(?i)<img(.|\\n)*?>')\n",
    "    # 去掉注释    \n",
    "    html = replace_str(html, '<!--(.|\\n)*?-->')       \n",
    "    # 去掉标签属性  \n",
    "    html = replace_str(html, '(?!&[a-z]+=)&[a-z]+;?', ' ') \n",
    "\n",
    "    return html\n",
    "\n",
    "# 处理soup\n",
    "def process_soup(soup):\n",
    "    # 去掉文本内容小于7的标签 # 比如<li><a href=\"http://www.gdtzb.com/kefu/show/33/\" id=\"top-1\">入网指导</a></li>的的入网指导长度小于7，舍去\n",
    "    for tag in soup.find_all():\n",
    "        \n",
    "        # 但是文本里不包括数字\n",
    "        if len(tag.get_text().strip()) < 7 and not re.search(r'\\d', tag.get_text()):\n",
    "            tag.decompose()\n",
    "\n",
    "    # 去掉空的标签\n",
    "    for tag in soup.find_all():\n",
    "\n",
    "        if tag.get_text().strip() == '':         \n",
    "            tag.decompose()\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "# 判断是否是所需要的item\n",
    "def is_item(soup):\n",
    "\n",
    "      # 如果存在大量相似的子节点，说明是li节点，直接返回soup      # 问题1     \n",
    "    child_type = list([child.name for child in soup.children if child.name is not None])     # 寻找child_type中出现次数最多的元素     \n",
    "    max_child_type = max(child_type, key = child_type.count)     \n",
    "    if child_type.count(max_child_type) > 4:         \n",
    "        return True\n",
    "\n",
    "\n",
    "def get_response(url):     \n",
    "    response = requests.get(url, headers=header)        \n",
    "    response.encoding = 'utf-8'  # 设置编码格式为utf-8     \n",
    "    return response\n",
    "\n",
    "\n",
    "url = \"http://www.zbwmy.com/150/151/index.html\"\n",
    "\n",
    "response = get_response(url)  # 获取网页源代码\n",
    "\n",
    "html = re_rules(response)  # 正则处理html\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')  \n",
    "\n",
    "soup = process_soup(soup)\n",
    "\n",
    "soup = recursive_process(soup)\n",
    "\n",
    "print(soup.prettify())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import numpy as np \n",
    "\n",
    "# # 特征提取：这里我们使用原始html标签\n",
    "# texts = [tag for tag in soup.find_all()] \n",
    "\n",
    "# print(texts)\n",
    "\n",
    "# 去掉文本内容小于7的标签 # 比如<li><a href=\"http://www.gdtzb.com/kefu/show/33/\" id=\"top-1\">入网指导</a></li>的的入网指导长度小于7，舍去\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# vectorizer = TfidfVectorizer(max_features=10) \n",
    "# X = vectorizer.fit_transform(texts)  # 应用K-均值聚类 \n",
    "# kmeans = KMeans(n_clusters=3) \n",
    "# kmeans.fit(X)  \n",
    "\n",
    "# # 输出聚类结果\n",
    "# for i in range(3):     \n",
    "#     print('第{}类:'.format(i))     \n",
    "#     for j in np.where(kmeans.labels_ == i)[0]:         \n",
    "#         print(texts[j]) \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'http://www.cqtic.net/cmssite/h/list/cms_xwzx_xwzx.jsp?id=895ea9a8-b5f4-4a22-952f-308008a02255&parentId=597dc378-b858-4076-8cd5-65791b9983a7&m=cms_xwzx_gkztb'\n",
    "\n",
    "response = requests.get(url, headers=header)\n",
    "\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "html = response.text\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37048\\377433912.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[1;31m# url = \"http://www.bidizhaobiao.com/advsearch/retrieval_list.do?searchCondition.SearchWord=%E4%B8%BB%E5%8F%98\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://history.sntba.com/website/news_list.aspx?category_id=53\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37048\\377433912.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;31m# response = get_response(url)  # 获取网页源码\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;31m# print(response.text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37048\\377433912.py\u001b[0m in \u001b[0;36mget_date\u001b[1;34m()\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mplaywright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_api\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msync_playwright\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[0mSTEALTH_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'stealth.min.js'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0msync_playwright\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m         \u001b[0mbrowser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchromium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m                  \u001b[0mheadless\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m                  \u001b[0mchromium_sandbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m                  \u001b[0mignore_default_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"--enable-automation\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                  \u001b[0mchannel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"chrome\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[0mua\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\python3\\lib\\site-packages\\playwright\\sync_api\\_context_manager.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m             raise Error(\n\u001b[0;32m     49\u001b[0m                 \"\"\"It looks like you are using Playwright Sync API inside the asyncio loop.\n\u001b[1;32m---> 50\u001b[1;33m Please use the Async API instead.\"\"\"\n\u001b[0m\u001b[0;32m     51\u001b[0m             )\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: It looks like you are using Playwright Sync API inside the asyncio loop.\nPlease use the Async API instead."
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup,Tag\n",
    "import numpy as np\n",
    "import requests \n",
    "import re  \n",
    "from typing import List\n",
    "import pymysql\n",
    "from IPython.display import clear_output\n",
    "from dateutil import parser\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299'\n",
    "}\n",
    "\n",
    "#  替换字符串\n",
    "def replace_str(source_str:str, regex:str, replace_str = '')->str:         \n",
    "     \n",
    "    str_info = re.compile(regex)         \n",
    "    return str_info.sub(replace_str, source_str)\n",
    "\n",
    "# 判断soup的子节点长度是否为1，是则递归调用，直到子节点长度不为1，返回soup\n",
    "def is_single_child_data(soup:Tag)->List[Tag]:\n",
    "    \n",
    "    children = list(soup.children)\n",
    "    num_children = [child for child in children if child.name is not None] \n",
    "         \n",
    "    if len(num_children) == 1:                  \n",
    "        return is_single_child_data(num_children[0])          \n",
    "    else:          \n",
    "        return num_children  \n",
    "\n",
    "# 获得soup最长的孩子节点\n",
    "def max_child_data(children:List[Tag])->Tag:\n",
    "        \n",
    "    max_child = None \n",
    "    max_length = 0 \n",
    "\n",
    "    # 遍历soup的所有直接子节点     \n",
    "    for child in children:         \n",
    "        if isinstance(child, Tag):  \n",
    "            # 确保它是一个标签     \n",
    "            cleaned_text = re.sub(r\"\\s+\", \"\", child.get_text(strip=True))        \n",
    "            text_length = len(cleaned_text)  # 获取去除空白后的文本长度   \n",
    "                 \n",
    "            if text_length > max_length:  \n",
    "                # 检查这是否是目前找到的最长的文本                 \n",
    "                max_length = text_length                 \n",
    "                max_child = child      \n",
    "    \n",
    "    return max_child  # 返回文本最长的子节点，如果没有找到则返回 None\n",
    "\n",
    "\n",
    "# 递归处理soup\n",
    "def recursive_process(soup:Tag,oldsoup:Tag)->Tag:     \n",
    "    # 检查是否只有一个子节点，如果是则深入     \n",
    "    soup = is_single_child_data(soup)  \n",
    "    \n",
    "    num = cal_item_std(soup)    # 计算newsoup标准差\n",
    "    if oldsoup:\n",
    "        oldnum = cal_item_std(oldsoup)  # 计算oldsoup标准差\n",
    "        # 最后一层，返回\n",
    "        if num == None:\n",
    "            return oldsoup\n",
    "        # num 和 oldnum 绝对值差小于1，返回\n",
    "        if num > oldnum - 3 and oldnum < 50 :    # 如果soup标准差大于oldsoup标准差，返回\n",
    "            return oldsoup\n",
    "  \n",
    "\n",
    "    # 在当前层找到最长的子节点     \n",
    "    new_soup = max_child_data(soup)   \n",
    "       \n",
    "    return recursive_process(new_soup,soup)\n",
    "\n",
    "# re正则处理html\n",
    "def re_rules(response:str)->str:\n",
    "\n",
    "    #如果含有body结构则正则获得body里的内容\n",
    "    if re.search(r'<body.*</body>', response, re.S):\n",
    "        html = re.search(r'<body.*</body>', response, re.S).group(0)\n",
    "    else:\n",
    "        html = response\n",
    "    \n",
    "    # 去掉script标签\n",
    "    html = replace_str(html, '(?i)<script(.|\\n)*?</script>') #(?i)忽略大小写   \n",
    "    # 去掉style标签    \n",
    "    html = replace_str(html, '(?i)<style(.|\\n)*?</style>')\n",
    "    # 去掉form标签(有些数据放到form里，坑！)\n",
    "    # html = replace_str(html, '(?i)<form(.|\\n)*?</form>') \n",
    "    # 去掉option标签\n",
    "    html = replace_str(html, '(?i)<option(.|\\n)*?</option>')\n",
    "    # 去掉input标签\n",
    "    html = replace_str(html, '(?i)<input(.|\\n)*?>') \n",
    "    # 去掉img标签\n",
    "    html = replace_str(html, '(?i)<img(.|\\n)*?>')\n",
    "    # 去掉注释    \n",
    "    html = replace_str(html, '<!--(.|\\n)*?-->')       \n",
    "    # 去掉标签属性  \n",
    "    html = replace_str(html, '(?!&[a-z]+=)&[a-z]+;?', ' ') \n",
    "\n",
    "    return html\n",
    "\n",
    "# 处理soup\n",
    "def process_soup(soup:Tag)->Tag:\n",
    "    # 去掉文本内容小于7的标签 # 比如<li><a href=\"http://www.gdtzb.com/kefu/show/33/\" id=\"top-1\">入网指导</a></li>的的入网指导长度小于7，舍去\n",
    "    for tag in soup.find_all('a'):\n",
    "        \n",
    "            # 但是文本里不包括数字\n",
    "            if len(tag.get_text().strip()) < 6 and not re.search(r'\\d', tag.get_text()):\n",
    "                tag.decompose()\n",
    "\n",
    "    # 去掉空的标签\n",
    "    for tag in soup.find_all():\n",
    "\n",
    "        if tag.get_text().strip() == '':         \n",
    "            tag.decompose()\n",
    "\n",
    "\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "# 判断是否是所需要的item\n",
    "def cal_item_std(children:List[Tag])->float:\n",
    "    \n",
    "    if not children:\n",
    "        return None\n",
    "\n",
    "    # 检查文本长度相似性     \n",
    "    text_lengths = [len(re.sub(r\"\\s+\", \"\", child.get_text(strip=True))) for child in children]\n",
    "\n",
    "    # 如果长度text_lengths大于5，省去最小的一个值,这个值可能是标题，分页等，干扰我们的判断\n",
    "    if len(text_lengths) > 5:\n",
    "        text_lengths.remove(min(text_lengths))\n",
    "\n",
    "    num = np.std(text_lengths) \n",
    "    \n",
    "    return num\n",
    "\n",
    "\n",
    "\n",
    "#  获取网页源码 get请求\n",
    "def get_response(url:str)->str:\n",
    "    response = requests.get(url, headers=header)   \n",
    "    response.encoding = 'utf-8'  # 设置编码格式为utf-8\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# 从mysql数据库获取url\n",
    "def urls_from_mysql()->List[str]:\n",
    "\n",
    "        host = '127.0.0.1'      \n",
    "        user = 'root'      \n",
    "        password = '123456'      \n",
    "        database = 'ceeg'     \n",
    "   \n",
    "        connection = pymysql.connect(         host=host,         user=user,         password=password,         database=database     )     \n",
    "        \n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        sql = \"select root_url from url_params_test_more where id < 15000\"  \n",
    "        cursor.execute(sql)    \n",
    "        urls = cursor.fetchall()\n",
    "\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "\n",
    "        return urls\n",
    "\n",
    "\n",
    "#  从文本中提取标题\n",
    "def find_title(text):          \n",
    "    title=re.findall('<title>(.+)</title>',text)          \n",
    "    return title\n",
    "\n",
    "# 添加http\n",
    "def start_http(baseurl,url):         \n",
    "    if url.startswith(\"http\"):             \n",
    "        return url        \n",
    "    else:   \n",
    "        if url.startswith(\"/\"):  \n",
    "            return baseurl + url[1:]   \n",
    "        else:\n",
    "            return baseurl + url\n",
    "\n",
    "\n",
    "#  解析url\n",
    "def parse_url(tag:Tag)->str:\n",
    "\n",
    "    # 如果tag是a标签，则直接处理该a标签     \n",
    "    if tag.name == 'a':         \n",
    "            a_tag = tag     \n",
    "    else:  \n",
    "            # 否则查找tag内部的所有a标签         \n",
    "            a_tag = tag.find_all('a') if tag.find_all('a') else None \n",
    "\n",
    "            if not a_tag:         \n",
    "                return None,None\n",
    "\n",
    "            # 获得a标签中文本最长的一个a的href                  \n",
    "            a_tag = max(a_tag, key = lambda x: len(x.get_text(strip=True)))\n",
    "\n",
    "\n",
    "    url = a_tag.get('href')\n",
    "    # 不纯在href属性，返回None\n",
    "    if not url:\n",
    "        return None,None\n",
    "     \n",
    "    cleaned_link = re.sub(r\"\\s+\", \"\", url)\n",
    "\n",
    "\n",
    "    title = a_tag.get_text(strip=True)\n",
    "\n",
    "    cleaned_title = re.sub(r\"\\s+\", \"\", title)  # 获得文本最长的title\n",
    "\n",
    "\n",
    "    return cleaned_link, cleaned_title\n",
    "\n",
    "# parser.parse\n",
    "def only_one_date(text:List)->datetime:\n",
    "\n",
    "    extracted_date = None\n",
    "    for key in text:         \n",
    "            try:\n",
    "                extracted_date = parser.parse(key, fuzzy=False).date()\n",
    "                # if extracted_date < datetime.now().date():\n",
    "\n",
    "                break         \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return extracted_date\n",
    "\n",
    "\n",
    "# parser.parse\n",
    "def many_date(text)->datetime:      \n",
    "    \n",
    "    extracted_date = []    \n",
    "    for key in text:                  \n",
    "        if(len(key) > 2):             \n",
    "            try:                 \n",
    "                extracted_date.append(parser.parse(key, fuzzy=False).date())    \n",
    "\n",
    "            except:                 \n",
    "                pass      \n",
    "       \n",
    "     # 返回最靠近目前时间的日期         \n",
    "    now = datetime.now()        \n",
    "    return min(extracted_date, key=lambda x: abs(x - now))\n",
    "\n",
    "\n",
    "\n",
    "def remove_chinese_characters(text:List[str])->List[str]:     \n",
    "    chinese_punctuation_pattern = r'[\\u3000-\\u303F\\uFF01-\\uFF0F\\uFF1A-\\uFF20\\uFF3B-\\uFF40\\uFF5B-\\uFF65]'  \n",
    "   \n",
    "\n",
    "    new_text = []\n",
    "    for item in text:         \n",
    "        # 正则表达式匹配汉字范围[\\u4e00-\\u9fa5]，使用re.sub函数替换匹配到的汉字为空字符串     \n",
    "        item = re.sub(r'[\\u4e00-\\u9fa5]', '', item) \n",
    "        item = re.sub(chinese_punctuation_pattern, '', item) \n",
    "        item = item.replace('[', '').replace(']', '').replace('...', '')\n",
    "        # 去掉空\n",
    "        item = re.sub(r'\\s+', '', item)\n",
    "        # 去掉年份\n",
    "        if len(item) == 4:\n",
    "            item = re.sub(r'\\d{4}', '', item)\n",
    "        if len(item) == 3:             \n",
    "            item = re.sub(r'\\d{3}', '', item)\n",
    "        if len(item) > 2 and item != '':             \n",
    "            new_text.append(item)\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "\n",
    "# 正则匹配日期   年月日\n",
    "def re_date(text:str)->datetime:\n",
    "    \n",
    "    pattern = r\"\"\"((?P<year>\\d{2,4})[-/.])((?P<month>\\d{1,2})[-/.])(?P<day>\\d{1,2})\"\"\"\n",
    "    regex = re.compile(pattern, re.VERBOSE)          \n",
    "    \n",
    "    # 匹配多个数据的话，返回最早的数据         \n",
    "    data_list = []\n",
    "\n",
    "    for match in regex.finditer(text):                    \n",
    "        year = match.group('year')             \n",
    "        month = match.group('month')             \n",
    "        day = match.group('day')\n",
    "\n",
    "        if year and month and day:\n",
    "            data_list.append(datetime.strptime(f\"{year}-{month}-{day}\", \"%Y-%m-%d\"))\n",
    "\n",
    "     # 返回最靠近目前时间的日期         \n",
    "    now = datetime.now()         \n",
    "    if data_list:              \n",
    "        return min(data_list, key=lambda x: abs(x - now)).date()       \n",
    "    else:             \n",
    "        return None\n",
    "\n",
    "# 正则匹配日期   月日,没有年份\n",
    "def re_dates(text:str)->datetime: \n",
    "\n",
    "    pattern = r\"\"\"((?P<month>\\d{1,2})[-/.])(?P<day>\\d{1,2})\"\"\"     \n",
    "    regex = re.compile(pattern, re.VERBOSE)                  \n",
    "      # 匹配多个数据的话，返回最早的数据              \n",
    "          \n",
    "    for match in regex.finditer(text):\n",
    "\n",
    "        year = match.group('year')                      \n",
    "        month = match.group('month')                      \n",
    "        day = match.group('day')          \n",
    "        if year and month and day:             \n",
    "            return datetime.strptime(f\"{year}-{month}-{day}\", \"%Y-%m-%d\").date()           \n",
    "        \n",
    "\n",
    "\n",
    "#  解析日期，title\n",
    "def parse_date(tag:Tag)->datetime:\n",
    "\n",
    "\n",
    "    text = tag.get_text()\n",
    "\n",
    "#    年月日齐全\n",
    "    date = re_date(text)\n",
    "#   年月日不齐全，只有月日，没有年份\n",
    "    if not date:\n",
    "        text = tag.get_text().split()\n",
    "        date = only_one_date(text)    \n",
    "\n",
    "\n",
    "    # # 每个子标签的文本内容提取出来\n",
    "    # text = [tag.get_text(strip=True) for tag in tag.find_all()] \n",
    "    # text = remove_chinese_characters(text)\n",
    "\n",
    "    # extracted_date = only_one_date(text) \n",
    "\n",
    "\n",
    "    # if not extracted_date:\n",
    "    #     # tag中有多个时间\n",
    "    #     extracted_date = many_date(text)\n",
    "  \n",
    "\n",
    "    return date\n",
    "\n",
    "\n",
    "\n",
    "# 获得url的base_url\n",
    "def get_base_url(url:str)->str:\n",
    "    #有http https\n",
    "\n",
    "    pattern = re.compile(r'(http://|https://)(.*?)/')\n",
    "\n",
    "    base_url = pattern.search(url).group(1) + pattern.search(url).group(2)+\"/\"\n",
    "\n",
    "    return base_url\n",
    "    \n",
    "\n",
    "# search关键词,是否包含关键词，包含则返回True\n",
    "def keyword(text:str)->bool:\n",
    "    \n",
    "    # 关键词列表\n",
    "    list = [\"油变\",\"油浸\",\"10KV\",\"35KV\",\"220KV\",\"落后产能淘汰\",\"主变\",\"中性点\",\"整流变\",\"配电变压器\",\"低压变压器\",\"高压变压器\",\"配变\",\"电力变压器\",\"变压器\",\"厂用变\",\"66KV\",\"开关柜\",\"环网柜\",\"中置柜\",\"高压柜\",\"低压柜\",\"变电站\",\"配电柜\",\"调压变压器\",\"节能变压器\",\"箱变\",\"电气柜数据\"]\n",
    "\n",
    "    # 如果list里的关键词在text里，则返回True\n",
    "    for i in list:\n",
    "        if i in text:\n",
    "            return True\n",
    "            \n",
    "    return False\n",
    "        \n",
    "\n",
    "# 过滤关键词,是否包含关键词，包含则返回True\n",
    "def filter_keyword(text:str)->bool:\n",
    "    \n",
    "     # 废弃关键词列表     \n",
    "    list =[\"中标\",\"成交\",\"候选人\",\"结果\",\"合同\",\"南网\",\"南方电网\",\"国网\",\"国家电网\",\"招标失败\",\"采购失败\",\"环境影响\",\"环评\",\"废标\",\"流标\",\"评标\",\"开标\",\"入围\",\"监理\",\"水土保持\",\"竣工\",\"单一来源\",\"单源直接采购\",\"勘察设计\",\"工程勘察\",\"验收\",\"备案\",\"审计\",\"审批\",\"评审\",\"核准\",\"受理\",\"批复\",\"政府采购意向\",\"地质灾害\",\"灾害危险性评估\",\"可研性\",\"可研初设\",\"可研设计\",\"可行性研究\",\"可行性报告\",\"预防性试验\",\"预防性检测\",\"预防性测试\",\"完成公示\",\"中止公告\",\"终止公告\",\"终止招标\",\"中选公告\",\"失败公告\",\"终止\",\"编制\",\"开标记录\",\"设计服务\",\"技术服务\",\"安全评估\",\"安全评价\",\"安全防护\",\"影响评价\",\"消防\",\"可研报告\",\"维护检修\",\"外委维护\",\"检修维护\",\"运行维护\",\"运维\",\"维保\",\"维修\",\"代维\",\"干式变压器\",\"干式配电变压器\",\"干式电力变压器\",\"控制变压器\",\"变压器处置\",\"挂牌\",\"转让\",\"拍卖\",\"变卖\",\"竞拍\",\"出让\",\"招租\",\"租赁\",\"医疗设备\",\"出租\",\"零星物资\",\"结算\",\"保险\",\"保洁\",\"劳保\",\"劳务分包\",\"劳务外包\",\"劳务招标\",\"外包项目\",\"服务外包\",\"施工专业承包\",\"消缺工\",\"程工程分包\",\"工程专业分包\",\"工程设计\",\"工程咨询\",\"工程造价咨询\",\"工程招标代理\",\"工程采购公示\",\"改造设计\",\"改造施工\",\"施工材料\",\"标识\",\"批前公示\",\"零星材料\",\"评审公示\",\"评价报告\",\"社会稳定风险评估\",\"水土保持报告\",\"影响评估\",\"泰开\",\"成套公司\",\"送变电公司\",\"电力电子公司\",\"物业管理\",\"空调\",\"监控\",\"电梯\",\"路灯专用\",\"灯具\",\"刀具\",\"夹具\",\"金具\",\"工具\",\"锁具\",\"家具\",\"五金\",\"消防物资\",\"配件\",\"附件\",\"硬件\",\"线材\",\"辅材\",\"耗材\",\"管材\",\"资产评估\",\"风险评估\",\"废旧物资处置\",\"无功补偿\",\"补偿装置\",\"补偿设备\",\"预算审核\",\"UPS\",\"GIS\",\"SVG\",\"AIS\",\"PHC\",\"AVC\",\"MPP\",\"KKS\",\"采购与安装\",\"采购及安装\",\"办公用品\",\"异常公告\",\"处置公告\",\"评标报告\",\"在线监测\",\"在线温度监测\",\"监测项目\",\"监测装置\",\"监测系统\",\"非物资\",\"风机塔筒\",\"医用\",\"体检\",\"餐饮\",\"射线\",\"导线\",\"母线\",\"装修\",\"工程类\",\"服务类\",\"迁改工程\",\"送出工程\",\"拆除工程\",\"电缆工程\",\"隔离开关\",\"矿用\",\"赛迪集团\",\"开滦集团\",\"内燃柴电机组\",\"灭火物资\",\"灭火系统\",\"灭火装置\",\"防火材料\",\"防火物资\",\"防火槽盒\", \"碎石机\"]\n",
    "    \n",
    "    for i in list:\n",
    "        if i in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#  时间范围,是否在时间范围内,在则返回True,不在则返回False\n",
    "def time_range(date):     \n",
    "    date_interval = 10     \n",
    "    current_date = datetime.now().date()     \n",
    "    target_date = date     \n",
    "    if target_date < current_date - timedelta(days=date_interval):         \n",
    "        return False    \n",
    "    return True \n",
    "\n",
    "# 匹配关键字，有效信息返回True\n",
    "def judge_content(text:str)->bool:\n",
    "    \n",
    "    # 是否包含关键词，不包含filter_keyword\n",
    "    if keyword(text):\n",
    "        if not filter_keyword(text):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_date(): \n",
    "\n",
    "    from playwright.sync_api import sync_playwright       \n",
    "    STEALTH_PATH = 'stealth.min.js'         \n",
    "    with sync_playwright() as p:             \n",
    "        browser = p.chromium.launch(                  headless=True,                  chromium_sandbox=False,                  ignore_default_args=[\"--enable-automation\"],                  channel=\"chrome\",)         \n",
    "        ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'                   \n",
    "        content = browser.new_context(user_agent=ua)              \n",
    "        content.add_init_script(path=STEALTH_PATH)                \n",
    "        page = content.new_page()                   \n",
    "        page.goto('https://search.bidcenter.com.cn/search?keywords=%e5%8f%98%e5%8e%8b%e5%99%a8')              \n",
    "        page.wait_for_timeout(5000)          \n",
    "        \n",
    "        return page.content()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run(url:str):\n",
    "        \n",
    "        \n",
    "        base_url = get_base_url(url)  # 获得url的base_url\n",
    "\n",
    "        # response = get_response(url)  # 获取网页源码 \n",
    "        response = get_date() \n",
    "        \n",
    "        # print(response.text)\n",
    "        html = re_rules(response)  # 正则处理html  \n",
    "        print(html)\n",
    "        soup = BeautifulSoup(html, 'html.parser')  # 创建soup对象  \n",
    "        soup = process_soup(soup) # 处理soup,空白字符，空标签等  \n",
    "        soup = recursive_process(soup,None) # 递归处理soup，找到最长的子节点，在递归处理，直到找到我们想要的item  \n",
    "        \n",
    "        for tag in soup:  \n",
    "            \n",
    "            # print(tag.preetify())\n",
    "            url,title = parse_url(tag)  # 解析url   \n",
    "            if not url:\n",
    "                continue\n",
    "            \n",
    "            url = start_http(base_url,url)  # 添加http\n",
    "\n",
    "            date = parse_date(tag)  # 获得日期,和标题   \n",
    "\n",
    "            # 如果日期为空，爬取详情页抓取时间\n",
    "            if not date:\n",
    "                continue\n",
    "                    # # 解析下一页\n",
    "                    # response = get_response(url)\n",
    "                    # html = re_rules(response)\n",
    "                    # date = re_dates(html)\n",
    "               \n",
    "            # # 如果爬取的日期太久，直接结束\n",
    "            # if not time_range(date): \n",
    "\n",
    "            #     break\n",
    "     \n",
    "            # if not judge_content(title):\n",
    "            #     continue\n",
    "\n",
    "\n",
    "            print(url,title,date)\n",
    "\n",
    "\n",
    "\n",
    "        # clear_output()  # 清空输出\n",
    "\n",
    "\n",
    "# urls = urls_from_mysql()     \n",
    "# for url in urls:         \n",
    "#     url = url[0]\n",
    "#     run(url)    \n",
    "\n",
    "\n",
    "# url = \"https://ec.powerchina.cn/zgdjcms//category/bulletinList.html?categoryId=2&page=1\"\n",
    "# url = \"https://www.gc-zb.com/search/index.html?keyword=%E5%8F%98%E5%8E%8B%E5%99%A8&h_lx=9&date=90&search_field=0&vague=0&h_province=0&submit=+\"\n",
    "# url = 'http://www.sxylcz.cn/list.php?cla=2&hy=0&addr=0'\n",
    "# url = \"https://www.bidnews.cn/search/\"\n",
    "# url = \"http://www.gdtzb.com/zb/search.php?kw=%E5%8F%98%E5%8E%8B%E5%99%A8&areaid=0&type=0\"\n",
    "# url = \"https://guangfu.bjx.com.cn/zb/\"\n",
    "# url = \"https://www.bidding-crmsc.com.cn/bid\"\n",
    "# url = \"https://cg.xidian.edu.cn/hwcggg/index.chtml\"\n",
    "# url = \"http://www.haikou.gov.cn/xxgk/szfbjxxgk/cztz/zfcg/cggg/\"\n",
    "# url = \"http://www.gzggzy.cn/jyywzfcgzfcgcggg/index.jhtml\"\n",
    "url = \"https://www.zbytb.com/zb/search.php?kw=%E5%8F%98%E5%8E%8B%E5%99%A8\"\n",
    "url = \"https://search.bidcenter.com.cn/search?keywords=%e5%8f%98%e5%8e%8b%e5%99%a8\"\n",
    "# url = \"https://www.chinabidding.cn/cblcn/Extension/index1\"\n",
    "# url = \"http://www.casc.com.cn/cas/?cat=13\"\n",
    "# url = \"http://www.jtgroup.com.cn/news\"\n",
    "# url = \"https://sist.shanghaitech.edu.cn/cggg/list.htm\"\n",
    "# url = \"http://58.221.160.226:82/webportal/index/bidnotice/list_t/2.do?cate=2\"\n",
    "# url = \"http://www.famens.com/Tradeb-.html\"\n",
    "# url = \"http://www.fjgczb.com/articleList.shtml?type=35\"\n",
    "# url = \"https://www.longdaoyun.com/search/business-opportunity/type-caigou.html\"\n",
    "url = \"https://www.cecbid.org.cn/tender/tender\"\n",
    "url = \"https://www.okcis.cn/bn/\"\n",
    "url = \"http://shanxi.chinabidding.cc/lists/pid/9/time/7.html\"\n",
    "# url = \"http://www.bidizhaobiao.com/advsearch/retrieval_list.do?searchCondition.SearchWord=%E4%B8%BB%E5%8F%98\"\n",
    "url = \"http://history.sntba.com/website/news_list.aspx?category_id=53\"\n",
    "run(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
